
@article{pilault_conditionally_2020,
	title = {Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in {NLP} Using Fewer Parameters \& Less Data},
	url = {https://www.semanticscholar.org/paper/Conditionally-Adaptive-Multi-Task-Learning%3A-in-NLP-Pilault-Elhattami/55c4a747855c74210919c45f7899e1f79e4c97f5},
	shorttitle = {Conditionally Adaptive Multi-Task Learning},
	abstract = {Multi-Task Learning ({MTL}) has emerged as a promising approach for transferring learned knowledge across different tasks. However, multi-task learning must deal with challenges such as: overfitting to low resource tasks, catastrophic forgetting, and negative task transfer, or learning interference. Additionally, in Natural Language Processing ({NLP}), {MTL} alone has typically not reached the performance level possible through per-task fine-tuning of pretrained models. However, many fine-tuning approaches are both parameter inefficient, e.g. potentially involving one new model per task, and highly susceptible to losing knowledge acquired during pretraining. We propose a novel transformer based architecture consisting of a new conditional attention mechanism as well as a set of task conditioned modules that facilitate weight sharing. Through this construction we achieve more efficient parameter sharing and mitigate forgetting by keeping half of the weights of a pretrained model fixed. We also use a new multi-task data sampling strategy to mitigate the negative effects of data imbalance across tasks. Using this approach we are able to surpass single-task fine-tuning methods while being parameter and data efficient. With our base model, we attain 2.2\% higher performance compared to a full fine-tuned {BERT} large model on the {GLUE} benchmark, adding only 5.6\% more trained parameters per task (whereas naive fine-tuning potentially adds 100\% of the trained parameters per task) and needing only 64.6\% of the data. We show that a larger variant of our single multi-task model approach performs competitively across 26 {NLP} tasks and yields state-of-the-art results on a number of test and development sets.},
	journaltitle = {{ArXiv}},
	author = {Pilault, Jonathan and Elhattami, Amine and Pal, C.},
	urldate = {2024-06-05},
	date = {2020-09-19},
}

@inproceedings{xinxi_single_2021,
	title = {Single task fine-tune {BERT} for text classification},
	volume = {11911},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11911/119111Z/Single-task-fine-tune-BERT-for-text-classification/10.1117/12.2604768.full},
	doi = {10.1117/12.2604768},
	abstract = {Over the past decades, natural language processing ({NLP}) has been a hot topic in many fields, e.g., sentiment analysis and news topic classification. As a very powerful language pre-training model, Bidirectional Encoder Representations from Transformers ({BERT}) has achieved promising results in many language understanding tasks including text classification. However, fine-tune {BERT} to adapt different text classification task efficiently is a critical problem that needs improvement. In this paper, a general solution is proposed for {BERT} fine-tuning on single text classification task. Compared with other traditional fine-tune strategies without any pre-training step, the performance of {BERT} is boosted by pre-training withintask data. Moreover, the proposed solution obtains superior results on six widely-used text classification datasets.},
	eventtitle = {2nd International Conference on Computer Vision, Image, and Deep Learning},
	pages = {434--439},
	booktitle = {2nd International Conference on Computer Vision, Image, and Deep Learning},
	publisher = {{SPIE}},
	author = {Xinxi, Zhang},
	urldate = {2024-06-05},
	date = {2021-10-05},
}

@book{carmona_conformance_2018,
	location = {Cham},
	title = {Conformance Checking: Relating Processes and Models},
	rights = {http://www.springer.com/tdm},
	isbn = {978-3-319-99413-0 978-3-319-99414-7},
	url = {http://link.springer.com/10.1007/978-3-319-99414-7},
	shorttitle = {Conformance Checking},
	publisher = {Springer International Publishing},
	author = {Carmona, Josep and Van Dongen, Boudewijn and Solti, Andreas and Weidlich, Matthias},
	urldate = {2024-06-04},
	date = {2018},
	langid = {english},
	doi = {10.1007/978-3-319-99414-7},
}

@misc{touvron_llama_2023,
	title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	shorttitle = {Llama 2},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models ({LLMs}) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned {LLMs}, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of {LLMs}.},
	number = {{arXiv}:2307.09288},
	publisher = {{arXiv}},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	urldate = {2024-06-03},
	date = {2023-07-19},
	eprinttype = {arxiv},
	eprint = {2307.09288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html},
	pages = {27730--27744},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F. and Leike, Jan and Lowe, Ryan},
	urldate = {2024-06-03},
	date = {2022-12-06},
	langid = {english},
}

@misc{zhou_enhancing_2024,
	title = {Enhancing the General Agent Capabilities of Low-Parameter {LLMs} through Tuning and Multi-Branch Reasoning},
	url = {http://arxiv.org/abs/2403.19962},
	doi = {10.48550/arXiv.2403.19962},
	abstract = {Open-source pre-trained Large Language Models ({LLMs}) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as {ChatGPT} and {GPT}-4. As intelligent agents, {LLMs} need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of {LLMs}. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the {LLMs}. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using {GPT}-4. Through supervised fine-tuning with constructed data, we find that for these models with a relatively small number of parameters, supervised fine-tuning can significantly reduce hallucination outputs and formatting errors in agent tasks. Furthermore, techniques such as multi-path reasoning and task decomposition can effectively decrease problem complexity and enhance the performance of {LLMs} as agents. We evaluate our method on five agent tasks of {AgentBench} and achieve satisfactory results.},
	number = {{arXiv}:2403.19962},
	publisher = {{arXiv}},
	author = {Zhou, Qinhao and Zhang, Zihan and Xiang, Xiang and Wang, Ke and Wu, Yuchuan and Li, Yongbin},
	urldate = {2024-06-03},
	date = {2024-03-28},
	eprinttype = {arxiv},
	eprint = {2403.19962 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{malta_my_2020,
	title = {My journey with {COVID}-19},
	volume = {27},
	issn = {2589-5370},
	url = {https://www.thelancet.com/journals/lanpub/article/PIIS2589-5370(20)30343-6/fulltext},
	doi = {10.1016/j.eclinm.2020.100599},
	journaltitle = {{eClinicalMedicine}},
	shortjournal = {{eClinicalMedicine}},
	author = {Malta, Monica},
	urldate = {2024-06-03},
	date = {2020-10-01},
	pmid = {33103091},
	note = {Publisher: Elsevier},
}

@inproceedings{brown_language_2020,
	title = {Language Models are Few-Shot Learners},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora.},
	pages = {1877--1901},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	urldate = {2024-06-03},
	date = {2020},
}

@article{kuo_rosacea_2015,
	title = {The rosacea patient journey: a novel approach to conceptualizing patient experiences},
	volume = {95},
	issn = {0011-4162},
	shorttitle = {The rosacea patient journey},
	abstract = {The motivation for rosacea patients to seek and adhere to treatment is not well characterized. A patient journey is a map of the steps a patient takes as he/she progresses through different segments of the disease from diagnosis to management, including all the influences that can push him/her toward or away from certain decisions. We sought to examine each step of the rosacea patient journey to better understand key patient care boundaries faced by rosacea patients. A {PubMed} search of articles indexed for {MEDLINE} as well as a search of the National Rosacea Society Web site (http://www.rosacea.org) were conducted to identify articles and materials that quantitatively or qualitatively described rosacea patient experiences. Current literature pertaining to the rosacea patient journey was summarized. The rosacea patient journey is discussed. It is a useful tool to gain insight on patient experiences. Better understanding of the patient perspective by dermatologists can lead to better patient adherence to treatment and thus improved quality of life and satisfaction.},
	pages = {37--43},
	number = {1},
	journaltitle = {Cutis},
	shortjournal = {Cutis},
	author = {Kuo, Sandy and Huang, Karen E. and Davis, Scott A. and Feldman, Steven R.},
	date = {2015-01},
	pmid = {25671443},
	keywords = {Humans, Medication Adherence, Patient Education as Topic, Rosacea, Self Concept},
}

@article{ferrara_engaging_2019,
	title = {Engaging the patient with {HCV} and Nash disease: a case study in 10 healthcare organizations in Italy},
	volume = {19},
	rights = {http://creativecommons.org/licenses/by/4.0},
	issn = {1568-4156},
	url = {https://www.ijic.org/article/10.5334/ijic.s3233/},
	doi = {10.5334/ijic.s3233},
	shorttitle = {Engaging the patient with {HCV} and Nash disease},
	pages = {233},
	number = {4},
	journaltitle = {International Journal of Integrated Care},
	shortjournal = {Int J Integr Care},
	author = {Ferrara, Lucia and Zazzera, Angelica and D Tozzi, Valeria},
	urldate = {2024-06-03},
	date = {2019-08-08},
	langid = {english},
}

@misc{madaan_self-refine_2023,
	title = {Self-Refine: Iterative Refinement with Self-Feedback},
	url = {http://arxiv.org/abs/2303.17651},
	doi = {10.48550/arXiv.2303.17651},
	shorttitle = {Self-Refine},
	abstract = {Like humans, large language models ({LLMs}) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from {LLMs} through iterative feedback and refinement. The main idea is to generate an initial output using an {LLMs}; then, the same {LLMs} provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single {LLM} as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art ({GPT}-3.5, {ChatGPT}, and {GPT}-4) {LLMs}. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same {LLM} using conventional one-step generation, improving by {\textasciitilde}20\% absolute on average in task performance. Our work demonstrates that even state-of-the-art {LLMs} like {GPT}-4 can be further improved at test time using our simple, standalone approach.},
	number = {{arXiv}:2303.17651},
	publisher = {{arXiv}},
	author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
	urldate = {2024-05-22},
	date = {2023-05-25},
	eprinttype = {arxiv},
	eprint = {2303.17651 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{munoz-gama_process_2022,
	title = {Process mining for healthcare: Characteristics and challenges},
	volume = {127},
	issn = {15320464},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046422000107},
	doi = {10.1016/j.jbi.2022.103994},
	shorttitle = {Process mining for healthcare},
	abstract = {Process mining techniques can be used to analyse business processes using the data logged during their execu­ tion. These techniques are leveraged in a wide range of domains, including healthcare, where it focuses mainly on the analysis of diagnostic, treatment, and organisational processes. Despite the huge amount of data generated in hospitals by staff and machinery involved in healthcare processes, there is no evidence of a systematic uptake of process mining beyond targeted case studies in a research context. When developing and using process mining in healthcare, distinguishing characteristics of healthcare processes such as their variability and patient-centred focus require targeted attention. Against this background, the Process-Oriented Data Science in Healthcare Alliance has been established to propagate the research and application of techniques targeting the data-driven improvement of healthcare processes. This paper, an initiative of the alliance, presents the distinguishing characteristics of the healthcare domain that need to be considered to successfully use process mining, as well as open challenges that need to be addressed by the community in the future.},
	pages = {103994},
	journaltitle = {Journal of Biomedical Informatics},
	shortjournal = {Journal of Biomedical Informatics},
	author = {Munoz-Gama, Jorge and Martin, Niels and Fernandez-Llatas, Carlos and Johnson, Owen A. and Sepúlveda, Marcos and Helm, Emmanuel and Galvez-Yanjari, Victor and Rojas, Eric and Martinez-Millana, Antonio and Aloini, Davide and Amantea, Ilaria Angela and Andrews, Robert and Arias, Michael and Beerepoot, Iris and Benevento, Elisabetta and Burattin, Andrea and Capurro, Daniel and Carmona, Josep and Comuzzi, Marco and Dalmas, Benjamin and De La Fuente, Rene and Di Francescomarino, Chiara and Di Ciccio, Claudio and Gatta, Roberto and Ghidini, Chiara and Gonzalez-Lopez, Fernanda and Ibanez-Sanchez, Gema and Klasky, Hilda B. and Prima Kurniati, Angelina and Lu, Xixi and Mannhardt, Felix and Mans, Ronny and Marcos, Mar and Medeiros De Carvalho, Renata and Pegoraro, Marco and Poon, Simon K. and Pufahl, Luise and Reijers, Hajo A. and Remy, Simon and Rinderle-Ma, Stefanie and Sacchi, Lucia and Seoane, Fernando and Song, Minseok and Stefanini, Alessandro and Sulis, Emilio and Ter Hofstede, Arthur H.M. and Toussaint, Pieter J. and Traver, Vicente and Valero-Ramon, Zoe and Weerd, Inge Van De and Van Der Aalst, Wil M.P. and Vanwersch, Rob and Weske, Mathias and Wynn, Moe Thandar and Zerbato, Francesca},
	urldate = {2024-05-05},
	date = {2022-03},
	langid = {english},
}

@misc{wei_zero-shot_2023,
	title = {Zero-Shot Information Extraction via Chatting with {ChatGPT}},
	url = {http://arxiv.org/abs/2302.10205},
	doi = {10.48550/arXiv.2302.10205},
	abstract = {Zero-shot information extraction ({IE}) aims to build {IE} systems from the unannotated text. It is challenging due to involving little human intervention. Challenging but worthwhile, zero-shot {IE} reduces the time and effort that data labeling takes. Recent efforts on large language models ({LLMs}, e.g., {GPT}-3, {ChatGPT}) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this work, we ask whether strong {IE} models can be constructed by directly prompting {LLMs}. Specifically, we transform the zero-shot {IE} task into a multi-turn question-answering problem with a two-stage framework ({ChatIE}). With the power of {ChatGPT}, we extensively evaluate our framework on three {IE} tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that {ChatIE} achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., {NYT}11-{HRL}). We believe that our work could shed light on building {IE} models with limited resources.},
	number = {{arXiv}:2302.10205},
	publisher = {{arXiv}},
	author = {Wei, Xiang and Cui, Xingyu and Cheng, Ning and Wang, Xiaobin and Zhang, Xin and Huang, Shen and Xie, Pengjun and Xu, Jinan and Chen, Yufeng and Zhang, Meishan and Jiang, Yong and Han, Wenjuan},
	urldate = {2024-05-03},
	date = {2023-02-20},
	eprinttype = {arxiv},
	eprint = {2302.10205 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{friedrich_process_2011,
	location = {Berlin, Heidelberg},
	title = {Process Model Generation from Natural Language Text},
	isbn = {978-3-642-21640-4},
	doi = {10.1007/978-3-642-21640-4_36},
	abstract = {Business process modeling has become an important tool for managing organizational change and for capturing requirements of software. A central problem in this area is the fact that the acquisition of as-is models consumes up to 60\% of the time spent on process management projects. This is paradox as there are often extensive documentations available in companies, but not in a ready-to-use format. In this paper, we tackle this problem based on an automatic approach to generate {BPMN} models from natural language text. We combine existing tools from natural language processing in an innovative way and augmented them with a suitable anaphora resolution mechanism. The evaluation of our technique shows that for a set of 47 text-model pairs from industry and textbooks, we are able to generate on average 77\% of the models correctly.},
	pages = {482--496},
	booktitle = {Advanced Information Systems Engineering},
	publisher = {Springer},
	author = {Friedrich, Fabian and Mendling, Jan and Puhlmann, Frank},
	editor = {Mouratidis, Haralambos and Rolland, Colette},
	date = {2011},
	langid = {english},
	keywords = {Business Process, Business Process Management, Natural Language Processing, Relative Clause, World Model, bpm, extraction},
}

@misc{han_is_2023,
	title = {Is Information Extraction Solved by {ChatGPT}? An Analysis of Performance, Evaluation Criteria, Robustness and Errors},
	url = {http://arxiv.org/abs/2305.14450},
	doi = {10.48550/arXiv.2305.14450},
	shorttitle = {Is Information Extraction Solved by {ChatGPT}?},
	abstract = {{ChatGPT} has stimulated the research boom in the field of large language models. In this paper, we assess the capabilities of {ChatGPT} from four perspectives including Performance, Evaluation Criteria, Robustness and Error Types. Specifically, we first evaluate {ChatGPT}'s performance on 17 datasets with 14 {IE} sub-tasks under the zero-shot, few-shot and chain-of-thought scenarios, and find a huge performance gap between {ChatGPT} and {SOTA} results. Next, we rethink this gap and propose a soft-matching strategy for evaluation to more accurately reflect {ChatGPT}'s performance. Then, we analyze the robustness of {ChatGPT} on 14 {IE} sub-tasks, and find that: 1) {ChatGPT} rarely outputs invalid responses; 2) Irrelevant context and long-tail target types greatly affect {ChatGPT}'s performance; 3) {ChatGPT} cannot understand well the subject-object relationships in {RE} task. Finally, we analyze the errors of {ChatGPT}, and find that "unannotated spans" is the most dominant error type. This raises concerns about the quality of annotated data, and indicates the possibility of annotating data with {ChatGPT}. The data and code are released at Github site.},
	number = {{arXiv}:2305.14450},
	publisher = {{arXiv}},
	author = {Han, Ridong and Peng, Tao and Yang, Chaohao and Wang, Benyou and Liu, Lu and Wan, Xiang},
	urldate = {2024-04-30},
	date = {2023-05-23},
	eprinttype = {arxiv},
	eprint = {2305.14450 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{li_exploring_2023,
	title = {Exploring Fine-tuning {ChatGPT} for News Recommendation},
	url = {http://arxiv.org/abs/2311.05850},
	doi = {10.48550/arXiv.2311.05850},
	abstract = {News recommendation systems ({RS}) play a pivotal role in the current digital age, shaping how individuals access and engage with information. The fusion of natural language processing ({NLP}) and {RS}, spurred by the rise of large language models such as the {GPT} and T5 series, blurs the boundaries between these domains, making a tendency to treat {RS} as a language task. {ChatGPT}, renowned for its user-friendly interface and increasing popularity, has become a prominent choice for a wide range of {NLP} tasks. While previous studies have explored {ChatGPT} on recommendation tasks, this study breaks new ground by investigating its fine-tuning capability, particularly within the news domain. In this study, we design two distinct prompts: one designed to treat news {RS} as the ranking task and another tailored for the rating task. We evaluate {ChatGPT}'s performance in news recommendation by eliciting direct responses through the formulation of these two tasks. More importantly, we unravel the pivotal role of fine-tuning data quality in enhancing {ChatGPT}'s personalized recommendation capabilities, and illustrates its potential in addressing the longstanding challenge of the "cold item" problem in {RS}. Our experiments, conducted using the Microsoft News dataset ({MIND}), reveal significant improvements achieved by {ChatGPT} after fine-tuning, especially in scenarios where a user's topic interests remain consistent, treating news {RS} as a ranking task. This study illuminates the transformative potential of fine-tuning {ChatGPT} as a means to advance news {RS}, offering more effective news consumption experiences.},
	number = {{arXiv}:2311.05850},
	publisher = {{arXiv}},
	author = {Li, Xinyi and Zhang, Yongfeng and Malthouse, Edward C.},
	urldate = {2024-04-26},
	date = {2023-11-09},
	eprinttype = {arxiv},
	eprint = {2311.05850 [cs]},
	keywords = {Computer Science - Information Retrieval},
}

@misc{wang_mitigating_2023,
	title = {Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites},
	url = {http://arxiv.org/abs/2312.01701},
	doi = {10.48550/arXiv.2312.01701},
	abstract = {Large language models ({LLMs}) have shown remarkable performance in natural language processing ({NLP}) tasks. To comprehend and execute diverse human instructions over image data, instruction-tuned large vision-language models ({LVLMs}) have been introduced. However, {LVLMs} may suffer from different types of object hallucinations. Nevertheless, {LVLMs} are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods. In this paper, we thus focus on reducing fine-grained hallucinations of {LVLMs}. We propose {\textbackslash}textit\{{ReCaption}\}, a framework that consists of two components: rewriting captions using {ChatGPT} and fine-tuning the instruction-tuned {LVLMs} on the rewritten captions. We also propose a fine-grained probing-based evaluation method named {\textbackslash}textit\{Fine-Grained Object Hallucination Evaluation\} ({\textbackslash}textit\{{FGHE}\}). Our experiment results demonstrate that {ReCaption} effectively reduces fine-grained object hallucination for different {LVLM} options and improves their text generation quality. The code can be found at https://github.com/Anonymousanoy/{FOHE}.},
	number = {{arXiv}:2312.01701},
	publisher = {{arXiv}},
	author = {Wang, Lei and He, Jiabang and Li, Shenshen and Liu, Ning and Lim, Ee-Peng},
	urldate = {2024-04-26},
	date = {2023-12-04},
	eprinttype = {arxiv},
	eprint = {2312.01701 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{gladkoff_predictive_2023,
	title = {Predictive Data Analytics with {AI}: assessing the need for post-editing of {MT} output by fine-tuning {OpenAI} {LLMs}},
	url = {http://arxiv.org/abs/2308.00158},
	doi = {10.48550/arXiv.2308.00158},
	shorttitle = {Predictive Data Analytics with {AI}},
	abstract = {Translation Quality Evaluation ({TQE}) is an essential step of the modern translation production process. {TQE} is critical in assessing both machine translation ({MT}) and human translation ({HT}) quality without reference translations. The ability to evaluate or even simply estimate the quality of translation automatically may open significant efficiency gains through process optimisation. This work examines whether the state-of-the-art large language models ({LLMs}) can be used for this purpose. We take {OpenAI} models as the best state-of-the-art technology and approach {TQE} as a binary classification task. On eight language pairs including English to Italian, German, French, Japanese, Dutch, Portuguese, Turkish, and Chinese, our experimental results show that fine-tuned gpt3.5 can demonstrate good performance on translation quality prediction tasks, i.e. whether the translation needs to be edited. Another finding is that simply increasing the sizes of {LLMs} does not lead to apparent better performances on this task by comparing the performance of three different versions of {OpenAI} models: curie, davinci, and gpt3.5 with 13B, 175B, and 175B parameters, respectively.},
	number = {{arXiv}:2308.00158},
	publisher = {{arXiv}},
	author = {Gladkoff, Serge and Erofeev, Gleb and Sorokina, Irina and Han, Lifeng and Nenadic, Goran},
	urldate = {2024-04-26},
	date = {2023-11-08},
	eprinttype = {arxiv},
	eprint = {2308.00158 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{ovadia_fine-tuning_2024,
	title = {Fine-Tuning or Retrieval? Comparing Knowledge Injection in {LLMs}},
	url = {http://arxiv.org/abs/2312.05934},
	doi = {10.48550/arXiv.2312.05934},
	shorttitle = {Fine-Tuning or Retrieval?},
	abstract = {Large language models ({LLMs}) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of {LLMs} on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation ({RAG}). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, {RAG} consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that {LLMs} struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.},
	number = {{arXiv}:2312.05934},
	publisher = {{arXiv}},
	author = {Ovadia, Oded and Brief, Menachem and Mishaeli, Moshik and Elisha, Oren},
	urldate = {2024-04-24},
	date = {2024-01-30},
	eprinttype = {arxiv},
	eprint = {2312.05934 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, fine-tuning},
}

@article{latif_fine-tuning_2024,
	title = {Fine-tuning {ChatGPT} for automatic scoring},
	volume = {6},
	issn = {2666920X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666920X24000110},
	doi = {10.1016/j.caeai.2024.100210},
	pages = {100210},
	journaltitle = {Computers and Education: Artificial Intelligence},
	shortjournal = {Computers and Education: Artificial Intelligence},
	author = {Latif, Ehsan and Zhai, Xiaoming},
	urldate = {2024-04-22},
	date = {2024-06},
	langid = {english},
	keywords = {fine-tuning},
}

@inproceedings{wu_matching_2023,
	location = {Cham},
	title = {Matching Exemplar as Next Sentence Prediction ({MeNSP}): Zero-Shot Prompt Learning for Automatic Scoring in Science Education},
	isbn = {978-3-031-36272-9},
	doi = {10.1007/978-3-031-36272-9_33},
	shorttitle = {Matching Exemplar as Next Sentence Prediction ({MeNSP})},
	abstract = {Developing natural language processing ({NLP}) models to automatically score students’ written responses to science problems is critical for science education. However, collecting sufficient student responses and labeling them for training or fine-tuning {NLP} models is time and cost-consuming. Recent studies suggest that large-scale pre-trained language models ({PLMs}) can be adapted to downstream tasks without fine-tuning by using prompts. However, no research has employed such a prompt approach in science education. As students’ written responses are presented with natural language, aligning the scoring procedure as the next sentence prediction task using prompts can skip the costly fine-tuning stage. In this study, we developed a zero-shot approach to automatically score student responses via Matching exemplars as Next Sentence Prediction ({MeNSP}). This approach employs no training samples. We first apply {MeNSP} in scoring three assessment tasks of scientific argumentation and found machine-human scoring agreements, Cohen’s Kappa ranges from 0.30 to 0.57, and F1 score ranges from 0.54 to 0.81. To improve scoring performance, we extend our research to the few-shots setting, either randomly selecting labeled student responses at each grading level or manually constructing responses to fine-tune the models. We find that one task’s performance is improved with more samples, Cohen’s Kappa from 0.30 to 0.38, and F1 score from 0.54 to 0.59; for the two other tasks, scoring performance is not improved. We also find that randomly selected few-shots perform better than the human expert-crafted approach. This study suggests that {MeNSP} can yield referable automatic scoring for student-written responses while significantly reducing the cost of model training. This method can benefit low-stakes classroom assessment practices in science education. Future research should further explore the applicability of the {MeNSP} in different types of assessment tasks in science education and further improve the model performance. Our code is available at https://github.com/{JacksonWuxs}/{MeNSP}.},
	pages = {401--413},
	booktitle = {Artificial Intelligence in Education},
	publisher = {Springer Nature Switzerland},
	author = {Wu, Xuansheng and He, Xinyu and Liu, Tianming and Liu, Ninghao and Zhai, Xiaoming},
	editor = {Wang, Ning and Rebolledo-Mendez, Genaro and Matsuda, Noboru and Santos, Olga C. and Dimitrova, Vania},
	date = {2023},
	langid = {english},
	keywords = {fine-tuning},
}

@misc{bertolini_automatic_2023,
	title = {Automatic Scoring of Dream Reports' Emotional Content with Large Language Models},
	url = {http://arxiv.org/abs/2302.14828},
	doi = {10.48550/arXiv.2302.14828},
	abstract = {In the field of dream research, the study of dream content typically relies on the analysis of verbal reports provided by dreamers upon awakening from their sleep. This task is classically performed through manual scoring provided by trained annotators, at a great time expense. While a consistent body of work suggests that natural language processing ({NLP}) tools can support the automatic analysis of dream reports, proposed methods lacked the ability to reason over a report's full context and required extensive data pre-processing. Furthermore, in most cases, these methods were not validated against standard manual scoring approaches. In this work, we address these limitations by adopting large language models ({LLMs}) to study and replicate the manual annotation of dream reports, using a mixture of off-the-shelf and bespoke approaches, with a focus on references to reports' emotions. Our results show that the off-the-shelf method achieves a low performance probably in light of inherent linguistic differences between reports collected in different (groups of) individuals. On the other hand, the proposed bespoke text classification method achieves a high performance, which is robust against potential biases. Overall, these observations indicate that our approach could find application in the analysis of large dream datasets and may favour reproducibility and comparability of results across studies.},
	number = {{arXiv}:2302.14828},
	publisher = {{arXiv}},
	author = {Bertolini, Lorenzo and Elce, Valentina and Michalak, Adriana and Bernardi, Giulio and Weeds, Julie},
	urldate = {2024-04-22},
	date = {2023-02-28},
	eprinttype = {arxiv},
	eprint = {2302.14828 [cs]},
	keywords = {Computer Science - Computation and Language, llm},
}

@article{weinstein_teaching_2018,
	title = {Teaching the science of learning},
	volume = {3},
	rights = {2018 The Author(s)},
	issn = {2365-7464},
	url = {https://cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-017-0087-y},
	doi = {10.1186/s41235-017-0087-y},
	abstract = {The science of learning has made a considerable contribution to our understanding of effective teaching and learning strategies. However, few instructors outside of the field are privy to this research. In this tutorial review, we focus on six specific cognitive strategies that have received robust support from decades of research: spaced practice, interleaving, retrieval practice, elaboration, concrete examples, and dual coding. We describe the basic research behind each strategy and relevant applied research, present examples of existing and suggested implementation, and make recommendations for further research that would broaden the reach of these strategies.},
	pages = {1--17},
	number = {1},
	journaltitle = {Cognitive Research: Principles and Implications},
	shortjournal = {Cogn. Research},
	author = {Weinstein, Yana and Madan, Christopher R. and Sumeracki, Megan A.},
	urldate = {2024-04-15},
	date = {2018-12},
	langid = {english},
	note = {Number: 1
Publisher: {SpringerOpen}},
	keywords = {learning},
}

@online{noauthor_fine-tuning_nodate,
	title = {Fine-Tuning {LLMs}: Overview, Methods \& Best Practices},
	url = {https://www.turing.com/resources/finetuning-large-language-models},
	urldate = {2024-04-13},
}

@book{van_der_aalst_process_2016,
	location = {Berlin, Heidelberg},
	title = {Process Mining},
	rights = {http://www.springer.com/tdm},
	isbn = {978-3-662-49850-7 978-3-662-49851-4},
	url = {http://link.springer.com/10.1007/978-3-662-49851-4},
	publisher = {Springer Berlin Heidelberg},
	author = {Van Der Aalst, Wil},
	urldate = {2024-04-12},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-662-49851-4},
}

@book{weske_business_2012,
	edition = {2},
	title = {Business Process Management},
	isbn = {978-3-642-28615-5},
	url = {https://link.springer.com/book/10.1007/978-3-642-28616-2},
	pagetotal = {{XVI}, 404},
	publisher = {Springer Berlin, Heidelberg},
	author = {Weske, Mathias},
	urldate = {2024-04-12},
	date = {2012-05-04},
	langid = {english},
	keywords = {business-process-management},
}

@misc{bahak_evaluating_2023,
	title = {Evaluating {ChatGPT} as a Question Answering System: A Comprehensive Analysis and Comparison with Existing Models},
	url = {http://arxiv.org/abs/2312.07592},
	doi = {10.48550/arXiv.2312.07592},
	shorttitle = {Evaluating {ChatGPT} as a Question Answering System},
	abstract = {In the current era, a multitude of language models has emerged to cater to user inquiries. Notably, the {GPT}-3.5 Turbo language model has gained substantial attention as the underlying technology for {ChatGPT}. Leveraging extensive parameters, this model adeptly responds to a wide range of questions. However, due to its reliance on internal knowledge, the accuracy of responses may not be absolute. This article scrutinizes {ChatGPT} as a Question Answering System ({QAS}), comparing its performance to other existing {QASs}. The primary focus is on evaluating {ChatGPT}'s proficiency in extracting responses from provided paragraphs, a core {QAS} capability. Additionally, performance comparisons are made in scenarios without a surrounding passage. Multiple experiments, exploring response hallucination and considering question complexity, were conducted on {ChatGPT}. Evaluation employed well-known Question Answering ({QA}) datasets, including {SQuAD}, {NewsQA}, and {PersianQuAD}, across English and Persian languages. Metrics such as F-score, exact match, and accuracy were employed in the assessment. The study reveals that, while {ChatGPT} demonstrates competence as a generative model, it is less effective in question answering compared to task-specific models. Providing context improves its performance, and prompt engineering enhances precision, particularly for questions lacking explicit answers in provided paragraphs. {ChatGPT} excels at simpler factual questions compared to "how" and "why" question types. The evaluation highlights occurrences of hallucinations, where {ChatGPT} provides responses to questions without available answers in the provided context.},
	number = {{arXiv}:2312.07592},
	publisher = {{arXiv}},
	author = {Bahak, Hossein and Taheri, Farzaneh and Zojaji, Zahra and Kazemi, Arefeh},
	urldate = {2024-04-04},
	date = {2023-12-11},
	eprinttype = {arxiv},
	eprint = {2312.07592 [cs]},
	keywords = {{AI}, {ChatGPT}},
}
