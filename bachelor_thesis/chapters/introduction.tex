\section{Introduction}\label{sec:intro}
Over recent years, Process Mining has proven to be a powerful tool for the detection, analysis, and optimization of (business) processes~\cite{weske_business_2012}. Its operation is fundamentally reliant on event logs, which traditionally are generated automatically within Information Systems.\\
However, with the rise of large language models (LLMs), interest in processing unstructured data has increased considerably. Should we be able to analyse not only structured data from companies, healthcare systems, for example, but also non-standard reports from arbitrary individuals, it would open up entirely new possibilities. These could range from analysing conversations from social media platforms such as Reddit or Facebook, to specifically requested reports that do not adhere to a strict format, making them easier for people to produce.\\
Consequently, there exists a crucial need to convert unstructured data into event logs, without which Process Mining cannot take place. For instance, mamahealth is currently working on a project that utilizes natural language testimonials from individuals with chronic illnesses. The project views these accounts from a process perspective, with the ultimate goal of generating potentially life-altering insights.\\
The project TracEX, developed in collaboration with mamahealth, plays an important role in this research and will be further introduced in the following chapters.It provides an extraction framework that this thesis aims to improve by providing fine-tuned models. There are significant challenges in sourcing and preparing data for Process Mining, even when using traditional data sources~\cite{van_der_aalst_process_2016}. From a practical standpoint, data quality is paramount for successful Process Mining. Any missing or untrustworthy event data severely undermines the value of the results obtained. 
\begin{quote}
    \quotes{From a practical point of view data quality is of the utmost importance for the success of process mining. If event data is missing or cannot be trusted, then the results of process mining are less valuable.}~\cite{van_der_aalst_process_2016}    
\end{quote}
These problems persist when unstructured text is utilized as a data source. Furthermore, the data has to be structured, which in turn complicates matters. Non-standardized, non-automated data, written by laypeople without any claim for completeness or readability, are even more prone to the issues faced with traditional data sources.\\
In the past, attempts have been made to process this type of data, including in the realm of process modelling~\cite{friedrich_process_2011}. Aside from  human manual extraction,  deterministic Natural Language Processing (NLP) approaches have been used. Unfortunately, these attempts were quickly met with limitations, particularly in deriving temporal relationships between events.\\
Recently, there has been an explosive rise in the field of Generative AI and LLMs in general. Transformer models from OpenAI, for instance, have proven to be powerful tools in various applications due to their ability to comprehend human language and respond in kind. This suggests that these models could also process textual disease course descriptions (patient journeys).\\
However, the use of LLMs introduces additional challenges. They are non-deterministic, meaning the same input can yield different outputs. Furthermore, their operation mechanisms are not transparent and therefore not entirely understood. Making them carry out tasks exactly as desired requires significant trial and error. For event logs, not only the quality of the collected data is vital, but so is the formatting. The data must not only be extracted accurately and completely, but also be cast into the appropriate format (such as XES).
These and many other hurdles pose substantial difficulties in event log extraction, as described in~\cite{munoz-gama_process_2022}. Established Transformer models from, as those from OpenAI, struggle to overcome these issues. Therefore, research has been conducted to explore ways to fine-tune these general-purpose models for specific tasks or topics. This work investigates the extent to which fine-tuning could potentially solve these problems and create high-quality event logs. As stated by~\cite{latif_fine-tuning_2024}, 
\begin{quote}
\quotes{Fine-tuned GPT models are more suited to tasks like text completion, response evaluation, or open-ended queries because of their autoregressive nature, which excels in sequence formation.} 
\end{quote}

This thesis is structured as follows:\\
Chapter~\ref{sec:back} introduces t
he preliminaries followed by Chapter~\ref{related_work}, situating this thesis among other studies. Chapter~\ref{sec:fine} explores different approaches to fine-tuning and describes the compilation of training data. In Chapter~\ref{sec:eval}, the performance of the trained models is evaluated after introducing the metrics required to do so. The thesis concludes with Chapter~\ref{sec:conclusion}, summarizing findings, discussing the limitations and providing an outlook for future work.