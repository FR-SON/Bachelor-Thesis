
\section{Preliminaries}\label{sec:back}
In this chapter, we introduce the terms and concepts used throughout the rest of the thesis. Where applicable, we use terms whose definitions are already established. At some points, we make minor adjustments to reduce complexity. 
\subsubsection*{Event Logs}\label{sec:event-log}
As the name suggests, event logs are collections of recorded events and their defining data related to an observed process. They can be used as input for process mining and, therefore, must adhere to a particular structure. Usually, they are represented as tables, where each row represents one event, and each column contains the values of the events' attributes. The bare minimum for these attributes is \emph{case ID} and \emph{activity}, assuming an event is made from a series of activities. Each instance of the process from which the events originate is labelled with an ID, the case ID. All events related to the same case are referred to as a \emph{trace}. An event log can contain multiple traces. On top of the mandatory attributes, timestamps, activity categories, and more process-specific data can be recorded in an event log.~\cite{van_der_aalst_process_2016}\\
In the context of this thesis, we consider events to be atomic, thus making the distinction between events and activities superfluous. Both terms can be used interchangeably.

\subsubsection*{Event}\label{sec:event}
An event consists of a case ID, an activity label, a start timestamp, and optionally other attributes such as the location where the event occurred.~\cite{van_der_aalst_process_2016} The activity label is a short description of the activity that took place. In the context of this thesis, it is not bound to a list of predefined terms or phrases. This means multiple activity labels can be syntactically different but semantically identical.

\subsubsection*{Patient Journeys}\label{sec:pj}
Patient journeys are natural language texts without any defined format or structure. However, there are properties, that are typically found in Patient Journeys: They describe a person's course of disease from the patient's perspective, as the patient themself usually writes it. It is agreed that the interaction between the patient and the health services, together with the following activities and interventions, are the critical components of a Patient Journey~\cite{ferrara_engaging_2019, kuo_rosacea_2015}. Characteristically, they encompass events spanning beyond, e.g. a single hospital stay, offering a broader picture by compiling experiences from before the onset of the condition, during hospital visits, at home, during doctor's appointments, and so on. Often, these accounts are enriched with personal feelings, either explicitly stated or implicitly conveyed through the writing style. The lack of structure and guidelines on what content must be included comes with advantages and disadvantages. Unlike doctor letters or automatic reports from Health Information Systems, they are more likely to be honest and approachable representations of how a patient perceives the course of disease.
On the other hand, Patient Journeys can have spelling and grammar issues, be of any length, and use mostly non-scient phrasing. This makes any two Patient Journeys difficult to compare. They are commonly found online, notably on social media platforms such as X, Facebook, and Reddit.

\subsubsection*{GPT-3.5}\label{sec:gpt3.5}
GPT-3.5 is an advanced language model developed by OpenAI, part of the GPT-3 series of models known for their capabilities in natural language processing. GPT-3.5 is designed to generate human-like text based on the input it receives, facilitating a wide range of applications from automated content creation to complex problem-solving tasks. Building on the architecture of its predecessors, GPT-3.5 utilizes a transformer-based model characterized by its use of self-attention mechanisms to balance the importance of various words in an input sequence.~\cite{latif_fine-tuning_2024} Within this framework, self-attention calculates attention scores for each word relative to others in a sequence, allowing the model to emphasize significant words during text generation. Multi-head attention further refines this capability by employing multiple attention heads to capture various types of relationships and dependencies. In academic and practical contexts, GPT-3.5 has been recognized for pushing the boundaries of what language models can achieve. Despite its advancements, ongoing research continues to address its limitations and explore ways to improve the model's robustness, fairness, and interpretability~\cite {brown_language_2020}. GPT-3.5 Turbo is an advanced version of GPT-3.5. We use it in this thesis due to its API availability and fitting balance between capabilities and affordability.

\subsubsection*{Fine-Tuning}\label{sec:fine-tuning-def}
Fine-tuning Large Language Models (LLMs) means customizing their operation to align with the specific demands of a domain or task.~\cite{ovadia_fine-tuning_2024} Predominantly, a pre-trained LLM, such as a ChatGPT model, forms the foundation for this process, which primarily entails continuous training or iterative updates to the model's knowledge base. The objective here is to optimize the model's ability to manage domain or task-specific requests, which could relate to either knowledge or response behaviour, including response format.\\
There are multiple fine-tuning methodologies to boost the potential of LLMs, including supervised fine-tuning~\cite{zhou_enhancing_2024}, unsupervised fine-tuning, and reinforcement learning methods.~\cite{touvron_llama_2023} \emph{Supervised Fine-Tuning} (SFT) employs labelled input-output pairs, which implies that each instance in the training dataset is equipped with the intended output that the algorithm should independently generate. \emph{Unsupervised fine-tuning} uses unlabelled datasets, meaning there is no ground truth element. The model processes the data without specific instructions. This implies that the model uses methods like clustering and anomaly detection to find structure or patterns in the data. In \emph{reinforcement learning}, a model strives to optimize its performance in a specific task. It is an iterative approach that includes multiple rounds of feedback and a reward that depends on the models' performance. Intending to find the strategy that yields the greatest overall reward, the model continuously adapts and improves.~\cite{ovadia_fine-tuning_2024}

\paragraph{Training Data}
When using SFT, the model learns solely from examples. It is given a system message, a user message, and an assistant message. The system message describes the behaviour the model should adopt and the task it should perform. The user message contains the input the model should perform the task on. The assistant message then contains the desired answer â€“ the response the model should give when confronted with this task in the future.
One message set closely resembles one turn of a regular exchange with ChatGPT, with the difference being that the answer is also given to the model instead of the model calculating the answer itself.
Example:
\begin{lstlisting} [language=json, caption={Example for fine-tuning training data}, label={lst:fine-tuning-example}]   
{"messages":[
    {"role":"system",
     "content":"You are an expert in text categorization. Your job is to take a given activity label and to classify it into one of the following event types: 'Symptom Onset', 'Symptom Offset', 'Diagnosis', 'Doctor Visit', 'Treatment', 'Hospital Admission', 'Hospital Discharge', 'Medication', 'Lifestyle Change' and 'Feelings'. Please consider the capitalization."
    },
    {"role":"user", "content":"testing positive for Covid19"},
    {"role":"assistant","content":"Diagnosis"}]}
\end{lstlisting}
The training data is made up of ten to hundreds of examples.

\subsubsection*{TracEX}\label{sec:tracex}
TracEX\footnote{https://github.com/FR-SON/TracEX} is an LLM-based data processing pipeline that aims to extract event logs from patient journeys. It takes a patient journey as an input and delivers an event log (in XES format) as an output. The extraction process involves many steps, some of which will be important later on. Hence, they are listed below:
\paragraph{Preprocessing} Patient Journeys are often unstructured, making it hard to extract information from them. Events are not always described in chronological order, time-related information is often missing or relative to each other (e.g. \quotes{two weeks later}), and language is not standardized (use of synonyms, paraphrases or slang). Extracting data from this reliably is virtually impossible. This is why preprocessing is essential. The patient journey is moulded into a more structured format in order to streamline the following steps while still retaining the continuous text format. This process includes correcting spelling and grammar as well as identifying and transforming time-related phrases into a date format (e.g. YYYY-MM-DD). Additionally, case information like the author's age and the described condition are extracted.
\paragraph{Extracting Event Types} Given an activity label, the category of that activity is determined from a predefined list. Possible event types are: Symptom Onset, Symptom Offset, Doctor Visit, Hospital Admission, Hospital Discharge, Treatment, Medication, Lifestyle Change and Feelings.
\paragraph{Labelling Activities} Unlike event types, activity labels are not chosen from a predefined list of possible activities. All relevant events are determined using the entire patient journey as context information and also considering the condition. They are then each summarized into six words at most. This implies that the exact phrasing of an activity can vary each time the pipeline is executed. An example of a typical activity is \quotes{experiencing fever-like symptoms}.
\paragraph{Extracting Timestamps} Given an activity, its start and end date are determined. It is extracted if possible and extrapolated if not. Furthermore, the duration of the activity is calculated from the two timestamps.\\\\
To achieve the described outcome, a mix of deterministic scripts and requests to the OpenAI API are used. Among other techniques, prompt engineering is used to increase the quality of the extracted event logs.
The Few-Shot prompting technique has proven to be especially suited for the described tasks. A request to the OpenAI API might contain a message list like this:
\begin{lstlisting}[language=json, caption={Few-shot prompt to categorize activities into event types}, label={lst:few-shot}]
{"messages":[
	{"role": "system",  
     "content": "You are an expert in text categorization and your job is to take a given activity label and to classify it into one of the following event types: 'Symptom Onset', 'Symptom Offset', 'Diagnosis', 'Doctor Visit', 'Treatment', 'Hospital Admission', 'Hospital Discharge', 'Medication', 'Lifestyle Change' and 'Feelings'. Please consider the capitalization.",  
	},  
    {"role": "user", "content": "visiting doctor's"},  
    {"role": "assistant", "content": "Doctor Visit"},  
    {"role": "user", "content": "tested positive for Covid19"},
    {"role": "assistant", "content": "Diagnosis"},  
    {"role": "user", "content": activity_label}  
]}
\end{lstlisting}
A prompt containing the task for the model and a list of examples to specify the models' behaviour is given, along with the activity label to be classified.
Please note that TracEX is an open-source tool. It may still be in development, and functionality might be altered, removed or added in the future. The development status this thesis builds upon can be checked 
\href{https://github.com/FR-SON/TracEX}{here}.
% \subsubsection*{ChatGPT}\label{sec:chatgpt}
% ChatGPT is an advanced conversational AI model developed by OpenAI. It signifies a considerable advancement in natural language processing (NLP). This powerful model is adept at understanding and generating human-like text, making it useful in a wide array of applications.\\
% Fundamentally, a large language model like ChatGPT is a statistical tool, trained to predict the probability distribution of word sequences. By analysing vast datasets of text, the model discerns patterns, structures, and semantic relationships within the data, enabling it to produce coherent and contextually suitable text.\\  
% ChatGPT is constructed on the Transformer architecture, which includes several key components. Notably, the attention mechanism plays a vital role. Within this framework, self-attention calculates attention scores for each word relative to others in a sequence, allowing the model to emphasize significant words during text generation. Multi-head attention further refines this capability by employing multiple attention heads to capture various types of relationships and dependencies.  Building on this foundation, GPT-3.5, an iteration of the model, enhances the scale and capabilities seen in its predecessor, GPT-3. Notably, GPT-3.5 inherits the expansive architecture of GPT-3, which has approximately 175 billion parameters. Parameters, essentially weights learned during the model's training, are crucial in determining its functionality and performance. GPT-3.5 Turbo harnesses extensive computational resources and large, diverse text corpora during its training.\\
% This process can be broken down into several critical phases. Pre-training involves exposing the model to a vast corpus of text data in an unsupervised manner, allowing it to learn language structures and patterns. Following this, fine-tuning adjusts the model's performance on specific tasks using labelled data. The final phase, optimization and deployment, involves rigorous testing and refinement to ensure efficiency and reliability in real-world applications.\\
% In operation, ChatGPT's mechanism starts with input processing, where user prompts are tokenized into a format the model can process. Tokens are subunits of text, such as words or subwords, that the model analyses. The generative process then involves these tokens being processed through numerous layers, ultimately generating probabilities for the next token in the sequence and decoding these generated tokens back into human-readable text.
