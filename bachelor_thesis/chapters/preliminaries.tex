
\section{Preliminaries}\label{sec:back}
In this chapter, we introduce the terms and concepts used throughout the rest of the thesis. Where applicable, we use terms whose definitions are already established. At some points, we make minor adjustments to reduce complexity.\\\\ 
The success of the approaches we present later in this thesis is evaluated by evaluating event logs. So this is the first term we introduce.
\subsubsection*{Event Logs and Events}\label{sec:event-log}
Our definitions are based on~\cite{van_der_aalst_process_2016}.
As the name suggests, event logs are collections of recorded events and their defining data related to an observed process. They can be used as input for process mining and, therefore, must adhere to a particular structure. Usually, they are represented as tables, where each row represents one event, and each column contains the values of the events' attributes. The bare minimum for these attributes is \emph{case ID} and \emph{activity} and a timestamp, to define an order between events. Each instance of the process from which the events originate is labelled with an ID, the case ID. All events related to the same case are referred to as a \emph{trace}.  An event log can contain one or multiple traces. Activity is a description of the event that occurred, also called an activity label. On top of the mandatory attributes, more process-specific attributes can be recorded in an event log.\\
An event consists of a case ID, an activity label, a start timestamp, an end timestamp, an event type, and optionally other attributes such as the location where the event occurred. The activity label is a short description of the activity that took place. In the context of this thesis, it is not bound to a list of predefined terms or phrases. This means multiple activity labels can be syntactically different but semantically identical. An event is uniquely identifiable by the combination of its attributes. If the activity label of two events is semantically very similar while also sharing the same start timestamp, these two events are indeed the same event.\\\\
The source we want to extract events from is Patient Journeys. Since this term has no widely accepted definition, we clarify below how we use it in this thesis.

\subsubsection*{Patient Journeys}\label{sec:pj}
Patient Journeys are natural language texts without any defined format or structure. However, there are properties, that are typically found in Patient Journeys: They describe a person's course of disease from the patient's perspective, as the patient themself usually writes it. It is agreed that the interaction between the patient and the health services, together with the following activities and interventions, are the critical components of a Patient Journey~\cite{ferrara_engaging_2019, kuo_rosacea_2015}. Characteristically, they encompass events spanning beyond, e.g. a single hospital stay, offering a broader picture by compiling experiences from before the onset of the condition, during hospital visits, at home, during doctor's appointments, and so on. Often, these accounts are enriched with personal feelings, either explicitly stated or implicitly conveyed through the writing style. The lack of structure and guidelines on what content must be included comes with advantages and disadvantages. Unlike doctor letters or automatic reports from Health Information Systems, they are more likely to be honest and approachable representations of how a patient perceives the course of disease.
On the other hand, Patient Journeys can have spelling and grammar issues, be of any length, and use mostly non-scientific phrasing. This makes any two Patient Journeys difficult to compare. They are commonly found online, notably on social media platforms such as X, Facebook, and Reddit.\\\\
The most essential tool in the extraction process we apply to create event logs from Patient Journeys is a GPT model. Therefore, we introduce the model we use in the following.

\subsubsection*{GPT-3.5}\label{sec:gpt3.5}
GPT-3.5 is an advanced language model developed by OpenAI, part of the GPT-3 series of models known for their capabilities in natural language processing. GPT-3.5 is designed to generate human-like text based on the input it receives, facilitating a wide range of applications from automated content creation to complex problem-solving tasks. Building on the architecture of its predecessors, GPT-3.5 utilizes a transformer-based model characterized by its use of self-attention mechanisms to balance the importance of various words in an input sequence.~\cite{latif_fine-tuning_2024} Within this framework, self-attention calculates attention scores for each word relative to others in a sequence, allowing the model to emphasize significant words during text generation. Multi-head attention further refines this capability by employing multiple attention heads to capture various types of relationships and dependencies. In academic and practical contexts, GPT-3.5 has been recognized for pushing the boundaries of what language models can achieve. Despite its advancements, ongoing research continues to address its limitations and explore ways to improve the model's robustness, fairness, and interpretability~\cite {brown_language_2020}. GPT-3.5 Turbo is an advanced version of GPT-3.5. We use it in this thesis due to its API availability and fitting balance between capabilities and affordability.\\\\
The focus of this thesis is fine-tuning. To provide common ground, we outline the fundamentals we need to discuss our work.

\subsubsection*{Fine-Tuning}\label{sec:fine-tuning-def}
Fine-tuning Large Language Models (LLMs) means customizing their operation to align with the specific demands of a domain or task.~\cite{ovadia_fine-tuning_2024} Predominantly, a pre-trained LLM, such as a ChatGPT model, forms the foundation for this process, which primarily entails continuous training or iterative updates to the model's knowledge base. The objective here is to optimize the model's ability to manage domain or task-specific requests, which could relate to either knowledge or response behaviour, including response format.\\
There are multiple fine-tuning methodologies to boost the potential of LLMs, including supervised fine-tuning~\cite{zhou_enhancing_2024}, unsupervised fine-tuning, and reinforcement learning methods.~\cite{touvron_llama_2023} \emph{Supervised Fine-Tuning} (SFT) employs labelled input-output pairs, which implies that each instance in the training dataset is equipped with the intended output that the algorithm should independently generate. \emph{Unsupervised fine-tuning} uses unlabelled datasets, meaning there is no ground truth element. The model processes the data without specific instructions. This implies that the model uses methods like clustering and anomaly detection to find structure or patterns in the data. In \emph{reinforcement learning}, a model strives to optimize its performance in a specific task. It is an iterative approach that includes multiple rounds of feedback and a reward that depends on the models' performance. Intending to find the strategy that yields the greatest overall reward, the model continuously adapts and improves~\cite{ovadia_fine-tuning_2024}.
Another dimension in which one can differentiate fine-tuning strategies is the number of tasks a model is fine-tuned for.\\
\emph{Multi-task fine-tuning} means training a model on multiple related tasks simultaneously – within one dataset. By leveraging the commonalities of the tasks, the model can learn and understand quicker, decreasing the number of examples it needs to increase its performance in all the tasks.~\cite{pilault_conditionally_2020}\\
\emph{Task-specific fine-tuning}, on the other hand, means training a model for only one well-defined, isolated task. This enables an even more refined understanding of the intricate details of one specific task. Especially with very complex tasks or those that require a deeper understanding of the domain, this method excels.~\cite{xinxi_single_2021}

\paragraph{Training Data}
When using SFT, the model learns solely from examples. It is given a system message, a user message, and an assistant message. The system message describes the behaviour the model should adopt and the task it should perform. The user message contains the input on which the model should perform the task. The assistant message then contains the desired answer – the response the model should give when confronted with this task in the future.
One message set closely resembles one turn of a regular exchange with ChatGPT, with the difference being that the answer is also given to the model instead of the model calculating the answer itself.
Example:
\begin{lstlisting} [language=json, caption={Example for fine-tuning training data}, label={lst:fine-tuning-example}]   
{"messages":[
    {"role":"system",
     "content":"You are an expert in text categorization. Your job is to take a given activity label and to classify it into one of the following event types: 'Symptom Onset', 'Symptom Offset', 'Diagnosis', 'Doctor Visit', 'Treatment', 'Hospital Admission', 'Hospital Discharge', 'Medication', 'Lifestyle Change' and 'Feelings'. Please consider the capitalization."
    },
    {"role":"user", "content":"testing positive for Covid19"},
    {"role":"assistant","content":"Diagnosis"}]}
\end{lstlisting}
The training data is made up of ten to hundreds of examples.\\\\
We use an open-source project as a foundation for our work. We use it as a reference to measure our accomplishments in this thesis. Because of that, we introduce the project's core functionalities and concepts.

\subsubsection*{TracEX}\label{sec:tracex}
TracEX is an LLM-based data processing pipeline that aims to extract event logs from patient journeys. It takes a patient journey as an input and delivers an event log as an output. The extraction process involves many steps, some of which will be important later on. Hence, they are listed below:
\paragraph{Preprocessing} Patient Journeys are often unstructured, making it hard to extract information from them. Events are not always described in chronological order, time-related information is often missing or relative to each other (e.g. \quotes{two weeks later}), and language is not standardized (use of synonyms, paraphrases or slang). Extracting data from this reliably is virtually impossible. This is why preprocessing is essential. The Patient Journey is moulded into a more structured format in order to streamline the following steps while still retaining the continuous text format. This process includes correcting spelling and grammar as well as identifying and transforming time-related phrases into a date format (e.g. YYYY-MM-DD). Additionally, case information like the author's age and the described condition are extracted.
\paragraph{Labelling Activities} All relevant events are determined using the entire Patient Journey as context information and also considering the condition. They are then each summarized into six words at most. This implies that the exact phrasing of an activity label can vary each time the pipeline is executed. An example of a typical activity is \quotes{experiencing fever-like symptoms}.
\paragraph{Extracting Event Types} Given an activity label, the category of that activity is determined from a predefined list. Possible event types are: Symptom Onset, Symptom Offset, Doctor Visit, Hospital Admission, Hospital Discharge, Treatment, Medication, Lifestyle Change and Feelings.
\paragraph{Extracting Timestamps} Given an activity label, its start and end date are determined. It is extracted if possible and extrapolated if not. Furthermore, the duration of the activity is calculated from the two timestamps.\\\\
To achieve the described outcome, a mix of deterministic scripts and requests to the OpenAI API are used. Among other techniques, prompt engineering is used to increase the quality of the extracted event logs.
The Few-Shot prompting technique has proven to be especially suited for the described tasks. A request to the OpenAI API might contain a message list like this:
\begin{lstlisting}[language=prompt, caption={Few-shot prompt to categorize activities into event types}, label={lst:few-shot}]
system: You are an expert in text categorization and your job is to take a given activity label and to classify it into one of the following event types: Symptom Onset, Symptom Offset, Diagnosis, Doctor Visit, Treatment, Hospital Admission, Hospital Discharge, Medication, Lifestyle Change and Feelings. Please consider the capitalization.  
user: visiting doctor's 
assistant: Doctor Visit  
user: tested positive for Covid19
assistant: Diagnosis
user: activity_label  
\end{lstlisting}
A prompt containing the task for the model and a list of examples to specify the models' behaviour is given, along with the activity label to be classified.
Please note that TracEX is an open-source tool. It may still be in development, and functionality might be altered, removed or added in the future. The development status this thesis builds upon can be checked in the repository\footnote{https://github.com/FR-SON/TracEX}.
