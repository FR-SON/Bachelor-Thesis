\section{Evaluation}\label{sec:eval}
In this chapter we will evaluate the presented approaches by using the trained models from Chapter~\ref{sec:multi-task-ft} and \ref{sec:single-task-ft} to extract event logs from patient journeys. In order to properly compare the fine-tuned models and the base model, we will first introduce custom metrics and a tool that will support the evaluation. Second, we will describe the evaluation process. Third, we will present our results for each model, each task and each of the task's metrics individually.\\
The evaluation is based on comparing the extracted event logs to a ground truth and determining the differences between them.

\subsection{Metrics}\label{sec:metrics}
In \ref{sec:multi-task-ft} we describe the process of fine-tuning a model to perform multiple related tasks. We evaluate each of these tasks individually. The model trained in \ref{sec:single-task-ft} performs only the task \quotes{Activity Labeling}, so we use the metric described in \ref{sec:activity_metrics} as well.

\subsubsection{Activity Labelling Metrics}\label{sec:activity_metrics}
There already are established metrics in the process mining sector we can use to determine if an event log is of good quality in terms of activities. Therefore, we just briefly explain how they work.\\
It is worth noting, that determining if one activity from the evaluated event log and one from the ground truth are indeed the same activity is not as straight forward as one might think. As discussed in \ref{sec:back} the activity labels are not predetermined, instead they are chosen by the model that extracts them. This means two activity labels can be syntactically different but still describe the same activity. For the purpose of this evaluation, we define two activities as identical, if they are semantically very similar.
\paragraph{Missing Activity} An activity is missing, if it is featured in the ground truth, but not in the evaluated event log. 
\paragraph{Unexpected Activity} An activity is unexpected, if it is featured in the evaluated event log, but not in the ground truth.
\paragraph{Wrong Order} Two activities are in wrong order, if both of them are featured in the ground truth as well as the evaluated event log, but their order of appearance is interchanged. In our case, the order of activities is defined by their start timestamp. This means two activities $A_1 \text{ and } A_2$ are in wrong order, if they appear in the order $A_1,A_2$ in the ground truth and in order $A_2, A_1$ in the evaluated event log.\\\\
As this is a comparative analysis, the value we use from this metric is the percentage of errors in each of the mentioned categories – Missing Activity, Unexpected Activity and Wrong order. This way, the number of errors is relative to the number of activities in the respective patient journey.

\subsubsection{Event Type Classification Metric}\label{sec:eventtype_metric}
We use a set of event types so each pair of them is mutually exclusive. This means for every activity in the ground truth there is exactly one correct event type. This fact reduces this metric to two possible results per event type: \emph{True} and \emph{False.}\\
The value we use from this metric is the percentage of \verb|True| and \verb|False| values respectively. This way, the number of errors is relative to the number of activities in the respective patient journey.\\
Possible event types are: Symptom Onset, Symptom Offset, Doctor Visit. Hospital Admission, Hospital Discharge, Treatment, Medication, Lifestyle Change, Feelings.

\subsubsection{Timestamp Extraction Metrics}\label{sec:time_metrics}
Similar to the event type metric, a timestamp can be either extracted correctly or incorrectly, resulting in \verb|True| or \verb|False| when evaluated. In order to adhere to the commonly used standard in event logs, that is also required to produce valid XES files, the timestamps are in the format \verb|YYYY-MM-DD|. In many cases, including the sample patient journeys we use in the evaluation, patient journeys do not contain time information this precise. Most time specifications are relative and vague. For the purpose of this evaluation, we define a timestamp to extracted correctly, if the date is plausible in the context of the patient journey. For an activity, that is described to have happened on \quotes{some day in April…}, any timestamp between 01.~April and 30.~April would be correct. This is, unless there is other time related information that further specifies the timeframe in which an activity must have happened. Let $A_1$ be an activity that happens before activity $A_2$ and $t_2=$ \verb|2024-04-15q| be the timestamp for $A_2$. If $A_1$ is specified as \quotes{some day in April…}, only timestamps between 01.~April and 14.~April would be correct. The same goes for \quotes{as the weeks progressed}, \quotes{later that year} and similar phrases. As long as an extracted timestamp is correct relatively to all other information given in the patient journey and is as precise as the text provides, it is deemed correct.\\
Start and end timestamps can be correct or incorrect independent of each other. The value we use from this metric is the percentage of \verb|True| and \verb|False| values respectively. This way, the number of errors is relative to the number of activities in the respective patient journey.

\subsection{Evaluation Process}\label{sec:eval_process}
In this chapter we will describe the process of evaluating the models trained in Chapter~\ref{sec:multi-task-ft} and \ref{sec:single-task-ft} using the metrics proposed in chapter \ref{sec:metrics}.\\
In order to evaluate the quality of the extraction performed by the various models, we need Patient Journeys to extract event logs from. First, we assemble a collection of Patient Journeys that pose different challenges. They are listed in full length in the appendix at \ref{apx:pjs}. They include well-structured descriptions of consecutive events and explicitly stated time stamps, but also implied activities and events with strong temporal relations that are not easily understood without taking a big context window into account. In short – the sample patient journeys provide a diverse mix of challenges, featuring both easy and hard to extract information.\\
We want to conduct comparative analyses between LLM models. To make these as fair and deterministic as possible, the next step is to create ground truths for every patient journey and every task that is performed on them. They can be found in the appendix at \ref{apx:ground_truth}. We later on use the same patient journeys and respective ground truth for all models.\\
The extraction itself is done using the TracEX pipeline. To address the dependency of the Event Type Classification step detailed in \ref{sec:back} we inject the activities from the ground truth into the pipeline before executing that step.This ensures that no previous step affects the results of the extraction in an uncontrolled way.\\\\
The extraction uses LLMs, which are non-deterministic by design. Due to that fact, the results vary between executions, even with the same input. To solve this issue, we perform each execution multiple times and use the mean of all values between the executions for the evaluation. All datasets without the mean calculation can be found in the appendix at \ref{apx:pipeline}.

\subsubsection{Evaluating Activity Labelling}\label{sec:eval_activity}
In the following we describe the process of evaluating the activity extraction capabilities of LLMs using the TracEX pipeline. The algorithm is implemented similarly in the Trace Testing Environment of TracEX.\\\\
For each activity of the event log, we look for a matching activity in the ground truth. To prevent false positives, we limit the number of activities to ±2 from the current activity. So an activity from the event log with index 5 would be compared to the activities with index 3 to 7 from the ground truth. As mentioned before, two activity labels have to be semantically very similar to be considered a match. This is done by prompting GPT4o, a much more capable model compared to GPT3.5 turbo, to rate the semantic similarity. The response from the OpenAI API also includes a \verb|logprobs| parameter, that indicates how confident the model is in its response. If no activity from the ground truth is a match, this activity is considered \emph{unexpected}. If exactly one activity from the ground truth is a match, that activity is considered correctly extracted. If multiple activities from the ground truth are matched, the one with the highest confidence score is used. All activities from the ground truth that remain without a match are considered \emph{missing}. Two activities from the event log, that have a matching activity in the ground truth each, but which's order is interchanged in the event log compared to the ground truth are counted as a \emph{wrong order} error.\\
The evaluation is automated to a large degree – but not perfect. We check every evaluation manually and adjust the results for the three metrics accordingly. The values for each metric can be checked for every run of the extraction pipeline for every model and every patient journey in the appendix at \ref{apx:metrics}

\subsubsection{Evaluating Event Type Classificartion}\label{sec:eval_event_type}
