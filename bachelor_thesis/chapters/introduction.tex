\section{Introduction}\label{sec:intro}
Over recent years, Process Mining has proven to be a powerful tool for detecting, analysing, and optimising (business) processes~\cite{weske_business_2012}. Its operation is fundamentally reliant on event logs, which traditionally are generated automatically within Information Systems.\\
However, with the rise of large language models (LLMs), interest in processing unstructured data has increased considerably. It would open up entirely new possibilities if we could analyse not only structured data from companies and healthcare systems, for example, but also non-standard reports from arbitrary individuals. These could range from analysing conversations from social media platforms such as Reddit or Facebook to specifically requested reports that do not adhere to a strict format, making them easier for people to produce.\\
Consequently, there is a crucial need to convert unstructured data into event logs, which is necessary to apply process mining. For instance, mamahealth is working on a project that utilises natural language testimonials from individuals with chronic illnesses. The project views these accounts from a process perspective, aiming to generate potentially life-altering insights.\\
The project TracEX, developed in collaboration with mamahealth, plays an essential role in this research and will be further introduced in the following chapters. It provides an extraction framework that this thesis aims to improve by providing fine-tuned models. There are significant challenges in sourcing and preparing data for Process Mining, even when using traditional data sources~\cite{van_der_aalst_process_2016}. From a practical standpoint, data quality is paramount for successful Process Mining. Any missing or untrustworthy event data severely undermines the value of the results obtained. 
\begin{quote}
    \quotes{From a practical point of view data quality is of the utmost importance for the success of process mining. If event data is missing or cannot be trusted, then the results of process mining are less valuable.}~\cite{van_der_aalst_process_2016}    
\end{quote}
These problems persist when using unstructured text as a data source. Furthermore, the data has to be structured, complicating matters even more. Non-standardised, non-automated data, written by laypeople without any claim for completeness or readability, are even more prone to the issues faced with traditional data sources.\\
In the past, attempts have been made to process this type of data, including in the realm of process modelling~\cite{friedrich_process_2011}. Aside from human manual extraction,  deterministic Natural Language Processing (NLP) approaches have been used. Unfortunately, these attempts quickly met limitations, particularly in deriving temporal relationships between events.\\
Recently, there has been an explosive rise in the field of Generative AI and LLMs in general. Transformer models from OpenAI, for instance, have proven to be powerful tools in various applications due to their ability to comprehend human language and respond in kind. This fact suggests that these models could also process textual disease course descriptions (patient journeys).\\
However, the use of LLMs introduces additional challenges. They are non-deterministic, meaning the same input can yield different outputs. Furthermore, their operation mechanisms are not transparent and, therefore, not entirely understood. Making them carry out tasks exactly as desired requires significant trial and error. For event logs, the quality and formatting of the collected data are vital. The data must not only be extracted accurately and completely but also cast into the appropriate format (such as XES).
These and many other hurdles pose substantial difficulties in event log extraction, as described in~\cite{munoz-gama_process_2022}. Established Transformer models, such as those from OpenAI, struggle to overcome these issues. Therefore, research has been conducted to explore ways to fine-tune these general-purpose models for specific tasks or topics. This work investigates the extent to which fine-tuning could solve these problems and create high-quality event logs. As stated by~\cite{latif_fine-tuning_2024}, 
\begin{quote}
\quotes{Fine-tuned GPT models are more suited to tasks like text completion, response evaluation, or open-ended queries because of their autoregressive nature, which excels in sequence formation.} 
\end{quote}

This thesis is structured as follows:\\
Chapter~\ref{sec:back} introduces t
he preliminaries followed by Chapter~\ref{related_work}, situating this thesis among other studies. Chapter~\ref{sec:fine} explores different approaches to fine-tuning and describes the compilation of training data. In Chapter~\ref{sec:eval}, the performance of the trained models is evaluated after introducing the required metrics. The thesis concludes with Chapter~\ref{sec:conclusion}, summarising findings, discussing the limitations and providing an outlook for future work.