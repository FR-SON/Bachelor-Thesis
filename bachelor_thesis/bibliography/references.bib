
@article{munoz-gama_process_2022,
	title = {Process mining for healthcare: Characteristics and challenges},
	volume = {127},
	issn = {15320464},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1532046422000107},
	doi = {10.1016/j.jbi.2022.103994},
	shorttitle = {Process mining for healthcare},
	abstract = {Process mining techniques can be used to analyse business processes using the data logged during their execu­ tion. These techniques are leveraged in a wide range of domains, including healthcare, where it focuses mainly on the analysis of diagnostic, treatment, and organisational processes. Despite the huge amount of data generated in hospitals by staff and machinery involved in healthcare processes, there is no evidence of a systematic uptake of process mining beyond targeted case studies in a research context. When developing and using process mining in healthcare, distinguishing characteristics of healthcare processes such as their variability and patient-centred focus require targeted attention. Against this background, the Process-Oriented Data Science in Healthcare Alliance has been established to propagate the research and application of techniques targeting the data-driven improvement of healthcare processes. This paper, an initiative of the alliance, presents the distinguishing characteristics of the healthcare domain that need to be considered to successfully use process mining, as well as open challenges that need to be addressed by the community in the future.},
	pages = {103994},
	journaltitle = {Journal of Biomedical Informatics},
	shortjournal = {Journal of Biomedical Informatics},
	author = {Munoz-Gama, Jorge and Martin, Niels and Fernandez-Llatas, Carlos and Johnson, Owen A. and Sepúlveda, Marcos and Helm, Emmanuel and Galvez-Yanjari, Victor and Rojas, Eric and Martinez-Millana, Antonio and Aloini, Davide and Amantea, Ilaria Angela and Andrews, Robert and Arias, Michael and Beerepoot, Iris and Benevento, Elisabetta and Burattin, Andrea and Capurro, Daniel and Carmona, Josep and Comuzzi, Marco and Dalmas, Benjamin and De La Fuente, Rene and Di Francescomarino, Chiara and Di Ciccio, Claudio and Gatta, Roberto and Ghidini, Chiara and Gonzalez-Lopez, Fernanda and Ibanez-Sanchez, Gema and Klasky, Hilda B. and Prima Kurniati, Angelina and Lu, Xixi and Mannhardt, Felix and Mans, Ronny and Marcos, Mar and Medeiros De Carvalho, Renata and Pegoraro, Marco and Poon, Simon K. and Pufahl, Luise and Reijers, Hajo A. and Remy, Simon and Rinderle-Ma, Stefanie and Sacchi, Lucia and Seoane, Fernando and Song, Minseok and Stefanini, Alessandro and Sulis, Emilio and Ter Hofstede, Arthur H.M. and Toussaint, Pieter J. and Traver, Vicente and Valero-Ramon, Zoe and Weerd, Inge Van De and Van Der Aalst, Wil M.P. and Vanwersch, Rob and Weske, Mathias and Wynn, Moe Thandar and Zerbato, Francesca},
	urldate = {2024-05-05},
	date = {2022-03},
	langid = {english},
}

@misc{wei_zero-shot_2023,
	title = {Zero-Shot Information Extraction via Chatting with {ChatGPT}},
	url = {http://arxiv.org/abs/2302.10205},
	doi = {10.48550/arXiv.2302.10205},
	abstract = {Zero-shot information extraction ({IE}) aims to build {IE} systems from the unannotated text. It is challenging due to involving little human intervention. Challenging but worthwhile, zero-shot {IE} reduces the time and effort that data labeling takes. Recent efforts on large language models ({LLMs}, e.g., {GPT}-3, {ChatGPT}) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this work, we ask whether strong {IE} models can be constructed by directly prompting {LLMs}. Specifically, we transform the zero-shot {IE} task into a multi-turn question-answering problem with a two-stage framework ({ChatIE}). With the power of {ChatGPT}, we extensively evaluate our framework on three {IE} tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that {ChatIE} achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., {NYT}11-{HRL}). We believe that our work could shed light on building {IE} models with limited resources.},
	number = {{arXiv}:2302.10205},
	publisher = {{arXiv}},
	author = {Wei, Xiang and Cui, Xingyu and Cheng, Ning and Wang, Xiaobin and Zhang, Xin and Huang, Shen and Xie, Pengjun and Xu, Jinan and Chen, Yufeng and Zhang, Meishan and Jiang, Yong and Han, Wenjuan},
	urldate = {2024-05-03},
	date = {2023-02-20},
	eprinttype = {arxiv},
	eprint = {2302.10205 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{friedrich_process_2011,
	location = {Berlin, Heidelberg},
	title = {Process Model Generation from Natural Language Text},
	isbn = {978-3-642-21640-4},
	doi = {10.1007/978-3-642-21640-4_36},
	abstract = {Business process modeling has become an important tool for managing organizational change and for capturing requirements of software. A central problem in this area is the fact that the acquisition of as-is models consumes up to 60\% of the time spent on process management projects. This is paradox as there are often extensive documentations available in companies, but not in a ready-to-use format. In this paper, we tackle this problem based on an automatic approach to generate {BPMN} models from natural language text. We combine existing tools from natural language processing in an innovative way and augmented them with a suitable anaphora resolution mechanism. The evaluation of our technique shows that for a set of 47 text-model pairs from industry and textbooks, we are able to generate on average 77\% of the models correctly.},
	pages = {482--496},
	booktitle = {Advanced Information Systems Engineering},
	publisher = {Springer},
	author = {Friedrich, Fabian and Mendling, Jan and Puhlmann, Frank},
	editor = {Mouratidis, Haralambos and Rolland, Colette},
	date = {2011},
	langid = {english},
	keywords = {Business Process, Business Process Management, Natural Language Processing, Relative Clause, World Model, bpm, extraction},
}

@misc{han_is_2023,
	title = {Is Information Extraction Solved by {ChatGPT}? An Analysis of Performance, Evaluation Criteria, Robustness and Errors},
	url = {http://arxiv.org/abs/2305.14450},
	doi = {10.48550/arXiv.2305.14450},
	shorttitle = {Is Information Extraction Solved by {ChatGPT}?},
	abstract = {{ChatGPT} has stimulated the research boom in the field of large language models. In this paper, we assess the capabilities of {ChatGPT} from four perspectives including Performance, Evaluation Criteria, Robustness and Error Types. Specifically, we first evaluate {ChatGPT}'s performance on 17 datasets with 14 {IE} sub-tasks under the zero-shot, few-shot and chain-of-thought scenarios, and find a huge performance gap between {ChatGPT} and {SOTA} results. Next, we rethink this gap and propose a soft-matching strategy for evaluation to more accurately reflect {ChatGPT}'s performance. Then, we analyze the robustness of {ChatGPT} on 14 {IE} sub-tasks, and find that: 1) {ChatGPT} rarely outputs invalid responses; 2) Irrelevant context and long-tail target types greatly affect {ChatGPT}'s performance; 3) {ChatGPT} cannot understand well the subject-object relationships in {RE} task. Finally, we analyze the errors of {ChatGPT}, and find that "unannotated spans" is the most dominant error type. This raises concerns about the quality of annotated data, and indicates the possibility of annotating data with {ChatGPT}. The data and code are released at Github site.},
	number = {{arXiv}:2305.14450},
	publisher = {{arXiv}},
	author = {Han, Ridong and Peng, Tao and Yang, Chaohao and Wang, Benyou and Liu, Lu and Wan, Xiang},
	urldate = {2024-04-30},
	date = {2023-05-23},
	eprinttype = {arxiv},
	eprint = {2305.14450 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{li_exploring_2023,
	title = {Exploring Fine-tuning {ChatGPT} for News Recommendation},
	url = {http://arxiv.org/abs/2311.05850},
	doi = {10.48550/arXiv.2311.05850},
	abstract = {News recommendation systems ({RS}) play a pivotal role in the current digital age, shaping how individuals access and engage with information. The fusion of natural language processing ({NLP}) and {RS}, spurred by the rise of large language models such as the {GPT} and T5 series, blurs the boundaries between these domains, making a tendency to treat {RS} as a language task. {ChatGPT}, renowned for its user-friendly interface and increasing popularity, has become a prominent choice for a wide range of {NLP} tasks. While previous studies have explored {ChatGPT} on recommendation tasks, this study breaks new ground by investigating its fine-tuning capability, particularly within the news domain. In this study, we design two distinct prompts: one designed to treat news {RS} as the ranking task and another tailored for the rating task. We evaluate {ChatGPT}'s performance in news recommendation by eliciting direct responses through the formulation of these two tasks. More importantly, we unravel the pivotal role of fine-tuning data quality in enhancing {ChatGPT}'s personalized recommendation capabilities, and illustrates its potential in addressing the longstanding challenge of the "cold item" problem in {RS}. Our experiments, conducted using the Microsoft News dataset ({MIND}), reveal significant improvements achieved by {ChatGPT} after fine-tuning, especially in scenarios where a user's topic interests remain consistent, treating news {RS} as a ranking task. This study illuminates the transformative potential of fine-tuning {ChatGPT} as a means to advance news {RS}, offering more effective news consumption experiences.},
	number = {{arXiv}:2311.05850},
	publisher = {{arXiv}},
	author = {Li, Xinyi and Zhang, Yongfeng and Malthouse, Edward C.},
	urldate = {2024-04-26},
	date = {2023-11-09},
	eprinttype = {arxiv},
	eprint = {2311.05850 [cs]},
	keywords = {Computer Science - Information Retrieval},
}

@misc{wang_mitigating_2023,
	title = {Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites},
	url = {http://arxiv.org/abs/2312.01701},
	doi = {10.48550/arXiv.2312.01701},
	abstract = {Large language models ({LLMs}) have shown remarkable performance in natural language processing ({NLP}) tasks. To comprehend and execute diverse human instructions over image data, instruction-tuned large vision-language models ({LVLMs}) have been introduced. However, {LVLMs} may suffer from different types of object hallucinations. Nevertheless, {LVLMs} are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods. In this paper, we thus focus on reducing fine-grained hallucinations of {LVLMs}. We propose {\textbackslash}textit\{{ReCaption}\}, a framework that consists of two components: rewriting captions using {ChatGPT} and fine-tuning the instruction-tuned {LVLMs} on the rewritten captions. We also propose a fine-grained probing-based evaluation method named {\textbackslash}textit\{Fine-Grained Object Hallucination Evaluation\} ({\textbackslash}textit\{{FGHE}\}). Our experiment results demonstrate that {ReCaption} effectively reduces fine-grained object hallucination for different {LVLM} options and improves their text generation quality. The code can be found at https://github.com/Anonymousanoy/{FOHE}.},
	number = {{arXiv}:2312.01701},
	publisher = {{arXiv}},
	author = {Wang, Lei and He, Jiabang and Li, Shenshen and Liu, Ning and Lim, Ee-Peng},
	urldate = {2024-04-26},
	date = {2023-12-04},
	eprinttype = {arxiv},
	eprint = {2312.01701 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{gladkoff_predictive_2023,
	title = {Predictive Data Analytics with {AI}: assessing the need for post-editing of {MT} output by fine-tuning {OpenAI} {LLMs}},
	url = {http://arxiv.org/abs/2308.00158},
	doi = {10.48550/arXiv.2308.00158},
	shorttitle = {Predictive Data Analytics with {AI}},
	abstract = {Translation Quality Evaluation ({TQE}) is an essential step of the modern translation production process. {TQE} is critical in assessing both machine translation ({MT}) and human translation ({HT}) quality without reference translations. The ability to evaluate or even simply estimate the quality of translation automatically may open significant efficiency gains through process optimisation. This work examines whether the state-of-the-art large language models ({LLMs}) can be used for this purpose. We take {OpenAI} models as the best state-of-the-art technology and approach {TQE} as a binary classification task. On eight language pairs including English to Italian, German, French, Japanese, Dutch, Portuguese, Turkish, and Chinese, our experimental results show that fine-tuned gpt3.5 can demonstrate good performance on translation quality prediction tasks, i.e. whether the translation needs to be edited. Another finding is that simply increasing the sizes of {LLMs} does not lead to apparent better performances on this task by comparing the performance of three different versions of {OpenAI} models: curie, davinci, and gpt3.5 with 13B, 175B, and 175B parameters, respectively.},
	number = {{arXiv}:2308.00158},
	publisher = {{arXiv}},
	author = {Gladkoff, Serge and Erofeev, Gleb and Sorokina, Irina and Han, Lifeng and Nenadic, Goran},
	urldate = {2024-04-26},
	date = {2023-11-08},
	eprinttype = {arxiv},
	eprint = {2308.00158 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@misc{ovadia_fine-tuning_2024,
	title = {Fine-Tuning or Retrieval? Comparing Knowledge Injection in {LLMs}},
	url = {http://arxiv.org/abs/2312.05934},
	doi = {10.48550/arXiv.2312.05934},
	shorttitle = {Fine-Tuning or Retrieval?},
	abstract = {Large language models ({LLMs}) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of {LLMs} on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation ({RAG}). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, {RAG} consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that {LLMs} struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.},
	number = {{arXiv}:2312.05934},
	publisher = {{arXiv}},
	author = {Ovadia, Oded and Brief, Menachem and Mishaeli, Moshik and Elisha, Oren},
	urldate = {2024-04-24},
	date = {2024-01-30},
	eprinttype = {arxiv},
	eprint = {2312.05934 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, fine-tuning},
}

@article{latif_fine-tuning_2024,
	title = {Fine-tuning {ChatGPT} for automatic scoring},
	volume = {6},
	issn = {2666920X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S2666920X24000110},
	doi = {10.1016/j.caeai.2024.100210},
	pages = {100210},
	journaltitle = {Computers and Education: Artificial Intelligence},
	shortjournal = {Computers and Education: Artificial Intelligence},
	author = {Latif, Ehsan and Zhai, Xiaoming},
	urldate = {2024-04-22},
	date = {2024-06},
	langid = {english},
	keywords = {fine-tuning},
}

@inproceedings{wu_matching_2023,
	location = {Cham},
	title = {Matching Exemplar as Next Sentence Prediction ({MeNSP}): Zero-Shot Prompt Learning for Automatic Scoring in Science Education},
	isbn = {978-3-031-36272-9},
	doi = {10.1007/978-3-031-36272-9_33},
	shorttitle = {Matching Exemplar as Next Sentence Prediction ({MeNSP})},
	abstract = {Developing natural language processing ({NLP}) models to automatically score students’ written responses to science problems is critical for science education. However, collecting sufficient student responses and labeling them for training or fine-tuning {NLP} models is time and cost-consuming. Recent studies suggest that large-scale pre-trained language models ({PLMs}) can be adapted to downstream tasks without fine-tuning by using prompts. However, no research has employed such a prompt approach in science education. As students’ written responses are presented with natural language, aligning the scoring procedure as the next sentence prediction task using prompts can skip the costly fine-tuning stage. In this study, we developed a zero-shot approach to automatically score student responses via Matching exemplars as Next Sentence Prediction ({MeNSP}). This approach employs no training samples. We first apply {MeNSP} in scoring three assessment tasks of scientific argumentation and found machine-human scoring agreements, Cohen’s Kappa ranges from 0.30 to 0.57, and F1 score ranges from 0.54 to 0.81. To improve scoring performance, we extend our research to the few-shots setting, either randomly selecting labeled student responses at each grading level or manually constructing responses to fine-tune the models. We find that one task’s performance is improved with more samples, Cohen’s Kappa from 0.30 to 0.38, and F1 score from 0.54 to 0.59; for the two other tasks, scoring performance is not improved. We also find that randomly selected few-shots perform better than the human expert-crafted approach. This study suggests that {MeNSP} can yield referable automatic scoring for student-written responses while significantly reducing the cost of model training. This method can benefit low-stakes classroom assessment practices in science education. Future research should further explore the applicability of the {MeNSP} in different types of assessment tasks in science education and further improve the model performance. Our code is available at https://github.com/{JacksonWuxs}/{MeNSP}.},
	pages = {401--413},
	booktitle = {Artificial Intelligence in Education},
	publisher = {Springer Nature Switzerland},
	author = {Wu, Xuansheng and He, Xinyu and Liu, Tianming and Liu, Ninghao and Zhai, Xiaoming},
	editor = {Wang, Ning and Rebolledo-Mendez, Genaro and Matsuda, Noboru and Santos, Olga C. and Dimitrova, Vania},
	date = {2023},
	langid = {english},
	keywords = {fine-tuning},
}

@misc{bertolini_automatic_2023,
	title = {Automatic Scoring of Dream Reports' Emotional Content with Large Language Models},
	url = {http://arxiv.org/abs/2302.14828},
	doi = {10.48550/arXiv.2302.14828},
	abstract = {In the field of dream research, the study of dream content typically relies on the analysis of verbal reports provided by dreamers upon awakening from their sleep. This task is classically performed through manual scoring provided by trained annotators, at a great time expense. While a consistent body of work suggests that natural language processing ({NLP}) tools can support the automatic analysis of dream reports, proposed methods lacked the ability to reason over a report's full context and required extensive data pre-processing. Furthermore, in most cases, these methods were not validated against standard manual scoring approaches. In this work, we address these limitations by adopting large language models ({LLMs}) to study and replicate the manual annotation of dream reports, using a mixture of off-the-shelf and bespoke approaches, with a focus on references to reports' emotions. Our results show that the off-the-shelf method achieves a low performance probably in light of inherent linguistic differences between reports collected in different (groups of) individuals. On the other hand, the proposed bespoke text classification method achieves a high performance, which is robust against potential biases. Overall, these observations indicate that our approach could find application in the analysis of large dream datasets and may favour reproducibility and comparability of results across studies.},
	number = {{arXiv}:2302.14828},
	publisher = {{arXiv}},
	author = {Bertolini, Lorenzo and Elce, Valentina and Michalak, Adriana and Bernardi, Giulio and Weeds, Julie},
	urldate = {2024-04-22},
	date = {2023-02-28},
	eprinttype = {arxiv},
	eprint = {2302.14828 [cs]},
	keywords = {Computer Science - Computation and Language, llm},
}

@article{weinstein_teaching_2018,
	title = {Teaching the science of learning},
	volume = {3},
	rights = {2018 The Author(s)},
	issn = {2365-7464},
	url = {https://cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-017-0087-y},
	doi = {10.1186/s41235-017-0087-y},
	abstract = {The science of learning has made a considerable contribution to our understanding of effective teaching and learning strategies. However, few instructors outside of the field are privy to this research. In this tutorial review, we focus on six specific cognitive strategies that have received robust support from decades of research: spaced practice, interleaving, retrieval practice, elaboration, concrete examples, and dual coding. We describe the basic research behind each strategy and relevant applied research, present examples of existing and suggested implementation, and make recommendations for further research that would broaden the reach of these strategies.},
	pages = {1--17},
	number = {1},
	journaltitle = {Cognitive Research: Principles and Implications},
	shortjournal = {Cogn. Research},
	author = {Weinstein, Yana and Madan, Christopher R. and Sumeracki, Megan A.},
	urldate = {2024-04-15},
	date = {2018-12},
	langid = {english},
	note = {Number: 1
Publisher: {SpringerOpen}},
	keywords = {learning},
}

@online{noauthor_fine-tuning_nodate,
	title = {Fine-Tuning {LLMs}: Overview, Methods \& Best Practices},
	url = {https://www.turing.com/resources/finetuning-large-language-models},
	urldate = {2024-04-13},
}

@book{van_der_aalst_process_2016,
	location = {Berlin, Heidelberg},
	title = {Process Mining},
	rights = {http://www.springer.com/tdm},
	isbn = {978-3-662-49850-7 978-3-662-49851-4},
	url = {http://link.springer.com/10.1007/978-3-662-49851-4},
	publisher = {Springer Berlin Heidelberg},
	author = {Van Der Aalst, Wil},
	urldate = {2024-04-12},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-662-49851-4},
}

@book{weske_business_2012,
	edition = {2},
	title = {Business Process Management},
	isbn = {978-3-642-28615-5},
	url = {https://link.springer.com/book/10.1007/978-3-642-28616-2},
	pagetotal = {{XVI}, 404},
	publisher = {Springer Berlin, Heidelberg},
	author = {Weske, Mathias},
	urldate = {2024-04-12},
	date = {2012-05-04},
	langid = {english},
	keywords = {business-process-management},
}

@misc{bahak_evaluating_2023,
	title = {Evaluating {ChatGPT} as a Question Answering System: A Comprehensive Analysis and Comparison with Existing Models},
	url = {http://arxiv.org/abs/2312.07592},
	doi = {10.48550/arXiv.2312.07592},
	shorttitle = {Evaluating {ChatGPT} as a Question Answering System},
	abstract = {In the current era, a multitude of language models has emerged to cater to user inquiries. Notably, the {GPT}-3.5 Turbo language model has gained substantial attention as the underlying technology for {ChatGPT}. Leveraging extensive parameters, this model adeptly responds to a wide range of questions. However, due to its reliance on internal knowledge, the accuracy of responses may not be absolute. This article scrutinizes {ChatGPT} as a Question Answering System ({QAS}), comparing its performance to other existing {QASs}. The primary focus is on evaluating {ChatGPT}'s proficiency in extracting responses from provided paragraphs, a core {QAS} capability. Additionally, performance comparisons are made in scenarios without a surrounding passage. Multiple experiments, exploring response hallucination and considering question complexity, were conducted on {ChatGPT}. Evaluation employed well-known Question Answering ({QA}) datasets, including {SQuAD}, {NewsQA}, and {PersianQuAD}, across English and Persian languages. Metrics such as F-score, exact match, and accuracy were employed in the assessment. The study reveals that, while {ChatGPT} demonstrates competence as a generative model, it is less effective in question answering compared to task-specific models. Providing context improves its performance, and prompt engineering enhances precision, particularly for questions lacking explicit answers in provided paragraphs. {ChatGPT} excels at simpler factual questions compared to "how" and "why" question types. The evaluation highlights occurrences of hallucinations, where {ChatGPT} provides responses to questions without available answers in the provided context.},
	number = {{arXiv}:2312.07592},
	publisher = {{arXiv}},
	author = {Bahak, Hossein and Taheri, Farzaneh and Zojaji, Zahra and Kazemi, Arefeh},
	urldate = {2024-04-04},
	date = {2023-12-11},
	eprinttype = {arxiv},
	eprint = {2312.07592 [cs]},
	keywords = {{AI}, {ChatGPT}},
}
