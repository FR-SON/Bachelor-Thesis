\section{Discussion}\label{sec:discussion}
In this chapter, we discuss the approaches and the results we achieved in hindsight.\\
Both approaches to fine-tuning improved GPT-3.5 Turbo's performance. In some metrics, there are noticeably fewer improvements, and we want to discuss that.\\
\begin{itemize}[partopsep=0pt, topsep=0pt]
    \item To our surprise, fine-tuning did not improve the model's capabilities in classifying event types for activities. This is the easiest of the four tasks we evaluated, and the base model performed reasonably well, setting the bar higher than in any other task. The reason for the smaller improvement is likely a lack of diversity in the training data. In many Patient Journeys, events of type Symptom Onset are more common than Treatment or Medication because patients can describe what they felt and experienced in more detail, compared to treatment and medication plans. Our training data reflects that.
    \item The single-task fine-tuned model performed worse or on par with the multi-task fine-tuned model in their mutual task. This is not necessarily an issue. The model learns from all the examples we provide during training and will use what it learned in every response. Consequently, the examples intended to train the model for Event Type Classification can also improve its performance in Activity Labelling. Even though the single-task fine-tuned model received twice as many examples for the task \quotes{Activity Labelling} than the multi-task fine-tuned model, the overall number of examples was lower. The conclusion is that multi-task fine-tuning, in this case, is the superior approach. More training data could tip the scale, however.
\end{itemize}
It is worth noting that the actual data reveals that the event logs from the fine-tuned models show improved consistency with lower variance between measurements.\\
The focus of \autoref{sec:fine} is selecting the correct training data and assembling data sets that the model can benefit from the most. We discussed various approaches and reasoned why one is better than the other. This relies on our experimentation, previous research, and claims made by OpenAI. Ultimately, all models we use are black-boxes, and there is no way to comprehend how exactly GPT-3.5 Turbo digests the data we provide. This is a major challenge for evaluation and comparing the efficacy of the approaches.