\section{Conclusion}\label{sec:conclusion}
This chapter completes the thesis. We will first discuss the limitations of the presented work and then outline the potential future work, that can make up for some of the limitations and extend the findings of our research.

\subsection{Limitations}\label{sec:limitations}
The approaches presented in \autoref{sec:fine} show some limitations. In the following paragraphs we disclose and discuss these limitations.\\\\
Attaining optimal results in extracting event logs from Patient Journeys with LLMs necessitates accounting for several critical factors. Foremost among these is the quality of input; substandard input invariably leads to substandard output. This factor however can usually not be controlled.\\\\
Ensuring a well-formulated prompt is imperative, as prompt engineering should invariably precede fine-tuning in efforts to enhance quality.
In this research, prompts are deliberately excluded as a variable, with the most current iterations from TracEX being employed. While these prompts are not without flaws and likely possess significant potential for enhancement, the main objective of this study is to assess the efficacy of fine-tuning in extracting event logs from patient journeys. This evaluation is conducted through a relative analysis approach; however, more definitive findings could be obtained if additional factors, such as the refinement of prompts, were also considered.\\\\
Fine-tuning is also intrinsically tied to the quantity of data available. Within the context of this bachelor’s thesis, the collection of training data is considerably constrained by time limitations. Absent these constraints, a greater quantity of data could be feasible, potentially leading to superior outcomes.\\\\
Both of the presented approaches to fine-tuning use supervised fine-tuning (SFT) techniques. As outlined in \autoref{sec:fine-tuning-def} there are more techniques we considered, and many more we did not consider. Ultimately, we decided to use OpenAI's models and services for our approaches, which focus on SFT. Other techniques such as Retrieval Augmented Generation, Reinforcement Learning\cite{ovadia_fine-tuning_2024} or Iterative Refinement with Self-Feedback~\cite{madaan_self-refine_2023} might also yield great results.

\subsection{Future Work}\label{sec:future_work}
We want to point out three aspects that can be addressed in future work.\\
Future investigations should further probe the potential ceiling of fine-tuning in the context we discussed. Using unstructured text as a source for process information will likely become more important. Though this study demonstrates that fine-tuning can yield improvements, its ultimate limitations remain undetermined. 
Using more training data and fine-tuning techniques as suggested in \autoref{sec:limitations} could be interesting.\\\\
Developing and applying even more distinguished metrics could further improve the fine-tuning process required for extracting from unstructured text. The metrics we introduced give us an indication for the performance of models on various tasks, but ultimately there is still a lot of information uncovered. Determining exactly what characteristics of the input restricts the model is therefore an interesting research topic.\\\\
Furthermore, as publicly accessible models continue to evolve — exemplified by the recent release of GPT-4o — the scope of possibilities extends correspondingly. By 2026, innovations that are presently inconceivable are likely to become achievable. Therefore, future research should build on the more powerful LLMs we will surely see in the upcoming years.

\subsection{Summary}\label{sec:summary}
Fine-tuning is an active research topic in the pursuit of increasing the performance of LLMs also in information extraction tasks. Extracting information from natural language text to create event logs poses some additional challenges, and Patient Journeys have proven to be inconsistent information sources.\\
We showed, that applying fine-tuning strategies to publicly available and affordable models can increase the quality of the extracted results, even with small amounts of training data. Especially noteworthy is the improvement in correctly determining and summarizing all relevant events in a specified context. We demonstrated that with Patient Journeys which describe Covid-19 courses of disease.\\
In combination with the successes in two of the other three evaluated tasks (extracting start and end timestamps), that both pose individual challenges, we conclude that fine-tuning, and SFT in particular, is indeed a valuable approach for improving the extraction of event logs from Patient Journeys.